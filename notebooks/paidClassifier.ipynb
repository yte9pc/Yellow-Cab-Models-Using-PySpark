{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project Assignment\n",
    "# Yihnew Eshetu (yte9pc), Nathan England (nle4bz), and Karyne Williams (kw7me)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.ml.linalg import DenseVector\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml.feature import VectorAssembler \n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import ChiSqSelector\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.regression import *\n",
    "from pyspark.ml import Pipeline \n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder \n",
    "from pyspark.ml.feature import *  \n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics, BinaryClassificationMetrics \n",
    "\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "#import plotly_express as px\n",
    "\n",
    "#sc = SparkContext.getOrCreate()\n",
    "#spark = SparkSession(sc)\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .master(\"local[*]\")\n",
    "    .appName('Paid Classifier')\n",
    "#     .config(“spark.executor.memory”, ‘15g’)\n",
    "#     .config(‘spark.executor.cores’, ‘1’)\n",
    "#     .config(‘spark.executor.instances’, ‘1’)\n",
    "     .config('spark.driver.memory','15g')\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Import and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function Reads Taxi Zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def taxiZone():\n",
    "    taxi = spark.read.csv(\"/sfs/qumulo/qhome/yte9pc/Project/Yellow_Cab_Data/Taxi_Zone/taxi_zone_lookup.csv\", header = True)\n",
    "    return taxi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function Reads Yellow Cab CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yellowCSVToDF():\n",
    "    location = \"/sfs/qumulo/qhome/yte9pc/Project/Yellow_Cab_Data/Data/Raw_Data/\"\n",
    "    \n",
    "\n",
    "    files = sorted(glob.glob(location + '*.csv'))\n",
    "\n",
    "    for idx,f in enumerate(files):\n",
    "        df = spark.read.csv(f, header = True)\n",
    "        print(f)\n",
    "\n",
    "        if idx == 0:\n",
    "            data = df\n",
    "        else:\n",
    "            data = data.union(df)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adds Taxi Zone for Pickup and Dropoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/sfs/qumulo/qhome/yte9pc/Project/Yellow_Cab_Data/Data/Raw_Data/yellow_tripdata_2019-05.csv\n",
      "/sfs/qumulo/qhome/yte9pc/Project/Yellow_Cab_Data/Data/Raw_Data/yellow_tripdata_2019-06.csv\n",
      "/sfs/qumulo/qhome/yte9pc/Project/Yellow_Cab_Data/Data/Raw_Data/yellow_tripdata_2019-07.csv\n",
      "/sfs/qumulo/qhome/yte9pc/Project/Yellow_Cab_Data/Data/Raw_Data/yellow_tripdata_2019-08.csv\n"
     ]
    }
   ],
   "source": [
    "def yellowTaxiZone():\n",
    "    # Drop Temp View\n",
    "    spark.catalog.dropTempView(\"taxiZone\")\n",
    "    spark.catalog.dropTempView(\"yellowCab\")\n",
    "    \n",
    "    taxiZone().createTempView(\"taxiZone\")\n",
    "    yellowCSVToDF().createTempView(\"yellowCab\")\n",
    "    yellowCab = spark.sql(\"SELECT yellowCab.*,\\\n",
    "                     CASE WHEN PU.Borough = 'Bronx'\\\n",
    "                          THEN 1\\\n",
    "                          WHEN PU.Borough = 'Brooklyn'\\\n",
    "                          THEN 2\\\n",
    "                          WHEN PU.Borough = 'EWR'\\\n",
    "                          THEN 3\\\n",
    "                          WHEN PU.Borough = 'Manhattan'\\\n",
    "                          THEN 4\\\n",
    "                          WHEN PU.Borough = 'Queens'\\\n",
    "                          THEN 5\\\n",
    "                          WHEN PU.Borough = 'Staten Island'\\\n",
    "                          THEN 6\\\n",
    "                          ELSE 7\\\n",
    "                     END as PUBorough,\\\n",
    "                     PU.Zone as PUZone,\\\n",
    "                     CASE WHEN PU.service_zone = 'Airports'\\\n",
    "                          THEN 1\\\n",
    "                          WHEN PU.service_zone = 'Boro Zone'\\\n",
    "                          THEN 2\\\n",
    "                          WHEN PU.service_zone = 'EWR'\\\n",
    "                          THEN 3\\\n",
    "                          WHEN PU.service_zone = 'Yellow Zone'\\\n",
    "                          THEN 4\\\n",
    "                          ELSE 5\\\n",
    "                     END as PUService_Zone,\\\n",
    "                     CASE WHEN DO.Borough = 'Bronx'\\\n",
    "                          THEN 1\\\n",
    "                          WHEN DO.Borough = 'Brooklyn'\\\n",
    "                          THEN 2\\\n",
    "                          WHEN DO.Borough = 'EWR'\\\n",
    "                          THEN 3\\\n",
    "                          WHEN DO.Borough = 'Manhattan'\\\n",
    "                          THEN 4\\\n",
    "                          WHEN DO.Borough = 'Queens'\\\n",
    "                          THEN 5\\\n",
    "                          WHEN DO.Borough = 'Staten Island'\\\n",
    "                          THEN 6\\\n",
    "                          ELSE 7\\\n",
    "                     END as DOBorough,\\\n",
    "                     DO.Zone as DOZone,\\\n",
    "                     CASE WHEN DO.service_zone = 'Airports'\\\n",
    "                          THEN 1\\\n",
    "                          WHEN DO.service_zone = 'Boro Zone'\\\n",
    "                          THEN 2\\\n",
    "                          WHEN DO.service_zone = 'EWR'\\\n",
    "                          THEN 3\\\n",
    "                          WHEN DO.service_zone = 'Yellow Zone'\\\n",
    "                          THEN 4\\\n",
    "                          ELSE 5\\\n",
    "                     END as DOService_Zone\\\n",
    "                     FROM yellowCab \\\n",
    "                     INNER JOIN taxiZone PU\\\n",
    "                            ON yellowCab.PULocationID = PU.LocationID \\\n",
    "                     INNER JOIN taxiZone DO\\\n",
    "                            ON yellowCab.DOLocationID = DO.LocationID\")\n",
    "    return yellowCab\n",
    "data = yellowTaxiZone()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropDuplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Column Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.withColumn(\"VendorID\", data[\"VendorID\"].cast(IntegerType()))\\\n",
    "    .withColumn(\"tpep_pickup_datetime\", data[\"tpep_pickup_datetime\"].cast(TimestampType()))\\\n",
    "    .withColumn(\"tpep_dropoff_datetime\", data[\"tpep_dropoff_datetime\"].cast(TimestampType()))\\\n",
    "    .withColumn(\"passenger_count\", data[\"passenger_count\"].cast(IntegerType()))\\\n",
    "    .withColumn(\"trip_distance\", data[\"trip_distance\"].cast(FloatType()))\\\n",
    "    .withColumn(\"RatecodeID\", data[\"RatecodeID\"].cast(IntegerType()))\\\n",
    "    .withColumn(\"PULocationID\", data[\"PULocationID\"].cast(IntegerType()))\\\n",
    "    .withColumn(\"DOLocationID\", data[\"DOLocationID\"].cast(IntegerType()))\\\n",
    "    .withColumn(\"payment_type\", data[\"payment_type\"].cast(IntegerType()))\\\n",
    "    .withColumn(\"fare_amount\", data[\"fare_amount\"].cast(FloatType()))\\\n",
    "    .withColumn(\"extra\", data[\"extra\"].cast(FloatType()))\\\n",
    "    .withColumn(\"mta_tax\", data[\"mta_tax\"].cast(FloatType()))\\\n",
    "    .withColumn(\"tip_amount\", data[\"tip_amount\"].cast(FloatType()))\\\n",
    "    .withColumn(\"tolls_amount\", data[\"tolls_amount\"].cast(FloatType()))\\\n",
    "    .withColumn(\"improvement_surcharge\", data[\"improvement_surcharge\"].cast(FloatType()))\\\n",
    "    .withColumn(\"total_amount\", data[\"total_amount\"].cast(FloatType()))\\\n",
    "    .withColumn(\"congestion_surcharge\", data[\"congestion_surcharge\"].cast(FloatType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Additional Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+---------------+---------------------+----------------+------------+-----------------+\n",
      "|tpep_pickup_datetime|pickup_hour|pickup_week_day|tpep_dropoff_datetime|dropoff_week_day|dropoff_hour|  trip_time(mins)|\n",
      "+--------------------+-----------+---------------+---------------------+----------------+------------+-----------------+\n",
      "| 2019-05-01 00:44:57|          0|              3|  2019-05-01 00:50:11|               3|           0|5.233333333333333|\n",
      "| 2019-05-01 00:59:57|          0|              3|  2019-05-01 01:13:18|               3|           1|            13.35|\n",
      "| 2019-05-01 00:09:43|          0|              3|  2019-05-01 00:14:36|               3|           0|4.883333333333334|\n",
      "+--------------------+-----------+---------------+---------------------+----------------+------------+-----------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = data.withColumn(\"pickup_week_day\", date_format(\"tpep_pickup_datetime\", \"u\").cast(IntegerType()))\\\n",
    "    .withColumn(\"pickup_hour\", hour(\"tpep_pickup_datetime\"))\\\n",
    "    .withColumn(\"dropoff_week_day\", date_format(\"tpep_dropoff_datetime\", \"u\").cast(IntegerType()))\\\n",
    "    .withColumn(\"dropoff_hour\", hour(\"tpep_dropoff_datetime\"))\\\n",
    "    .withColumn(\"trip_time(mins)\", (col(\"tpep_dropoff_datetime\").cast(LongType()) - col(\"tpep_pickup_datetime\").cast(LongType()))/60)\\\n",
    "    .withColumn(\"distance/time\", (col(\"trip_distance\")/col(\"trip_time(mins)\")))\n",
    "data.select([\"tpep_pickup_datetime\", \"pickup_hour\", \"pickup_week_day\", \"tpep_dropoff_datetime\", \n",
    "             \"dropoff_week_day\", \"dropoff_hour\", \"trip_time(mins)\"]).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop na"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- tpep_pickup_datetime: timestamp (nullable = true)\n",
      " |-- tpep_dropoff_datetime: timestamp (nullable = true)\n",
      " |-- passenger_count: integer (nullable = true)\n",
      " |-- trip_distance: float (nullable = true)\n",
      " |-- RatecodeID: integer (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- payment_type: integer (nullable = true)\n",
      " |-- fare_amount: float (nullable = true)\n",
      " |-- extra: float (nullable = true)\n",
      " |-- mta_tax: float (nullable = true)\n",
      " |-- tip_amount: float (nullable = true)\n",
      " |-- tolls_amount: float (nullable = true)\n",
      " |-- improvement_surcharge: float (nullable = true)\n",
      " |-- total_amount: float (nullable = true)\n",
      " |-- congestion_surcharge: float (nullable = true)\n",
      " |-- PUBorough: integer (nullable = false)\n",
      " |-- PUZone: string (nullable = true)\n",
      " |-- PUService_Zone: integer (nullable = false)\n",
      " |-- DOBorough: integer (nullable = false)\n",
      " |-- DOZone: string (nullable = true)\n",
      " |-- DOService_Zone: integer (nullable = false)\n",
      " |-- pickup_week_day: integer (nullable = true)\n",
      " |-- pickup_hour: integer (nullable = true)\n",
      " |-- dropoff_week_day: integer (nullable = true)\n",
      " |-- dropoff_hour: integer (nullable = true)\n",
      " |-- trip_time(mins): double (nullable = true)\n",
      " |-- distance/time: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove cash and credit card payment where total amount is less than 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.filter(((data.total_amount >= 0) & (data.payment_type <= 2)) | (data.payment_type > 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove rides with trip time less than 0 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.filter((col('trip_time(mins)') >= 0) & (col('trip_distance') >= 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We will construct a model to determine whether or not a passenger will pay. \n",
    "What factors determine whether or not a passenger will pay? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Model Specific Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Unknown Payment Types and Voided Trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Payment type: 5 = Unknown, 6 = Voided Trip\n",
    "data = data.filter(col(\"payment_type\") < 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create label to identify whether or not a passenger paid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.withColumn('paid', F.when(col(\"payment_type\") <= 2, 1).otherwise(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+\n",
      "|paid|   count|\n",
      "+----+--------+\n",
      "|   1|17743064|\n",
      "|   0|  141077|\n",
      "+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.groupBy('paid').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balance Data by Upsampling Negatives and Downsampling Positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "negatives = data.filter(data['paid'] == 0)\n",
    "positives = data.filter(data['paid'] == 1)\n",
    "\n",
    "#Downsample positives to 1% of orignal data size\n",
    "pos_undersampled = positives.sample(withReplacement = False, fraction = .01, seed = 123)\n",
    "\n",
    "neg_count = negatives.count()\n",
    "pos_count = pos_undersampled.count() \n",
    "ratio = pos_count/neg_count\n",
    "\n",
    "neg_oversampled = negatives.sample(withReplacement=True, fraction=ratio, seed = 123 )\n",
    "data = pos_undersampled.unionAll(neg_oversampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+\n",
      "|paid| count|\n",
      "+----+------+\n",
      "|   1|176927|\n",
      "|   0|176830|\n",
      "+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.groupBy('paid').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Bench Mark Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: integer (nullable = false)\n",
      " |-- trip_distance: float (nullable = true)\n",
      " |-- fare_amount: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vars_to_keep = ['trip_distance', 'fare_amount']\n",
    "bm_data = data.select(col('paid').alias('label'), *vars_to_keep)\n",
    "bm_data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Splitting: Training and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "(bm_train, bm_test) = bm_data.randomSplit([.8, .2], seed = 123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "va = VectorAssembler(inputCols= vars_to_keep, outputCol=\"assembled_features\")\n",
    "scale = StandardScaler(inputCol= va.getOutputCol(), outputCol=\"features\")\n",
    "lr = LogisticRegression(featuresCol = scale.getOutputCol(), labelCol = 'label', maxIter=10)\n",
    "\n",
    "# Build the pipeline\n",
    "bm_stages = [va, scale, lr]\n",
    "bm_pipeline = Pipeline(stages= bm_stages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [0.1, 0.01]) \\\n",
    "    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]) \\\n",
    "    .build()\n",
    "\n",
    "crossval = CrossValidator(estimator= bm_pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=BinaryClassificationEvaluator(),\n",
    "                          numFolds=10)\n",
    "\n",
    "# Run cross-validation, and choose the best set of parameters.\n",
    "bm_cvModel = crossval.fit(bm_train)\n",
    "\n",
    "# Make a prediction\n",
    "bm_prediction = bm_cvModel.transform(bm_test).select('label', 'prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: integer (nullable = false)\n",
      " |-- prediction: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bm_prediction.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm_preds_and_labels = bm_prediction.select(['prediction','label']).withColumn('label', bm_prediction['label'].cast(FloatType())).orderBy('prediction')\n",
    "\n",
    "#select only prediction and label columns\n",
    "bm_preds_and_labels = bm_preds_and_labels.select(['prediction','label']).rdd.map(tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "[[24856. 10308.]\n",
      " [20517. 14815.]]\n"
     ]
    }
   ],
   "source": [
    "bm_multi_metrics = MulticlassMetrics(bm_preds_and_labels)\n",
    "bm_bin_metrics = BinaryClassificationMetrics(bm_preds_and_labels)\n",
    "print(\"Confusion Matrix\")\n",
    "print(bm_multi_metrics.confusionMatrix().toArray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary Stats\n",
      "Accuracy = 0.5627411484339537\n",
      "Precision = 0.5896986824821876\n",
      "Recall = 0.4193082757839918\n",
      "F1 Score = 0.4901166156645439\n",
      "Area under ROC = 0.5630837818460399\n"
     ]
    }
   ],
   "source": [
    "print(\"Summary Stats\")\n",
    "print(\"Accuracy = %s\" % bm_multi_metrics.accuracy)\n",
    "print(\"Precision = %s\" % bm_multi_metrics.precision(1.0))\n",
    "print(\"Recall = %s\" % bm_multi_metrics.recall(1.0))\n",
    "print(\"F1 Score = %s\" % bm_multi_metrics.fMeasure(1.0))\n",
    "print(\"Area under ROC = %s\" % bm_bin_metrics.areaUnderROC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Complex Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: integer (nullable = false)\n",
      " |-- passenger_count: integer (nullable = true)\n",
      " |-- trip_distance: float (nullable = true)\n",
      " |-- RatecodeID: integer (nullable = true)\n",
      " |-- fare_amount: float (nullable = true)\n",
      " |-- tolls_amount: float (nullable = true)\n",
      " |-- improvement_surcharge: float (nullable = true)\n",
      " |-- congestion_surcharge: float (nullable = true)\n",
      " |-- PUBorough: integer (nullable = false)\n",
      " |-- pickup_week_day: integer (nullable = true)\n",
      " |-- pickup_hour: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "comp_vars_to_keep = ['passenger_count', 'trip_distance', 'RatecodeID', 'fare_amount', 'tolls_amount', \n",
    "                    'improvement_surcharge', 'congestion_surcharge', 'PUBorough', 'pickup_week_day', 'pickup_hour']\n",
    "comp_data = data.select(col('paid').alias('label'), *comp_vars_to_keep)\n",
    "comp_data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Splitting: Training and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "(comp_train, comp_test) = comp_data.randomSplit([.8, .2], seed = 123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "va = VectorAssembler(inputCols= comp_vars_to_keep, outputCol=\"assembled_features\")\n",
    "scale = StandardScaler(inputCol= va.getOutputCol(), outputCol=\"features\")\n",
    "lr = LogisticRegression(featuresCol = scale.getOutputCol(), labelCol = 'label', maxIter=10)\n",
    "\n",
    "# Build the pipeline\n",
    "comp_stages = [va, scale, lr]\n",
    "comp_pipeline = Pipeline(stages= comp_stages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [0.1, 0.01]) \\\n",
    "    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]) \\\n",
    "    .build()\n",
    "\n",
    "crossval = CrossValidator(estimator= comp_pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=BinaryClassificationEvaluator(),\n",
    "                          numFolds=10)\n",
    "\n",
    "# Run cross-validation, and choose the best set of parameters.\n",
    "comp_cvModel = crossval.fit(comp_train)\n",
    "\n",
    "# Make a prediction\n",
    "comp_prediction = comp_cvModel.transform(comp_test).select('label', 'prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: integer (nullable = false)\n",
      " |-- prediction: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "comp_prediction.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_preds_and_labels = comp_prediction.select(['prediction','label']).withColumn('label', comp_prediction['label'].cast(FloatType())).orderBy('prediction')\n",
    "\n",
    "#select only prediction and label columns\n",
    "comp_preds_and_labels = comp_preds_and_labels.select(['prediction','label']).rdd.map(tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "[[14705. 20459.]\n",
      " [ 2910. 32422.]]\n"
     ]
    }
   ],
   "source": [
    "comp_multi_metrics = MulticlassMetrics(comp_preds_and_labels)\n",
    "comp_bin_metrics = BinaryClassificationMetrics(comp_preds_and_labels)\n",
    "print(\"Confusion Matrix\")\n",
    "print(comp_multi_metrics.confusionMatrix().toArray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary Stats\n",
      "Accuracy = 0.6685060145256468\n",
      "Precision = 0.6131124600518144\n",
      "Recall = 0.9176384014491112\n",
      "F1 Score = 0.7350843979912257\n",
      "Area under ROC = 0.6679108854020668\n"
     ]
    }
   ],
   "source": [
    "print(\"Summary Stats\")\n",
    "print(\"Accuracy = %s\" % comp_multi_metrics.accuracy)\n",
    "print(\"Precision = %s\" % comp_multi_metrics.precision(1.0))\n",
    "print(\"Recall = %s\" % comp_multi_metrics.recall(1.0))\n",
    "print(\"F1 Score = %s\" % comp_multi_metrics.fMeasure(1.0))\n",
    "print(\"Area under ROC = %s\" % comp_bin_metrics.areaUnderROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook /sfs/qumulo/qhome/nle4bz/ds5559/Project/Tree Model.ipynb to pdf\n",
      "[NbConvertApp] Writing 80141 bytes to ./notebook.tex\n",
      "[NbConvertApp] Building PDF\n",
      "[NbConvertApp] Running xelatex 3 times: ['xelatex', './notebook.tex', '-quiet']\n",
      "[NbConvertApp] Running bibtex 1 time: ['bibtex', './notebook']\n",
      "[NbConvertApp] WARNING | bibtex had problems, most likely because there were no citations\n",
      "[NbConvertApp] PDF successfully created\n",
      "[NbConvertApp] Writing 80555 bytes to /sfs/qumulo/qhome/nle4bz/ds5559/Project/Tree Model.pdf\n",
      "[NbConvertApp] Converting notebook /sfs/qumulo/qhome/nle4bz/ds5559/Project/dimensionaModeling.ipynb to pdf\n",
      "[NbConvertApp] Writing 77525 bytes to ./notebook.tex\n",
      "[NbConvertApp] Building PDF\n",
      "[NbConvertApp] Running xelatex 3 times: ['xelatex', './notebook.tex', '-quiet']\n",
      "[NbConvertApp] Running bibtex 1 time: ['bibtex', './notebook']\n",
      "[NbConvertApp] WARNING | bibtex had problems, most likely because there were no citations\n",
      "[NbConvertApp] PDF successfully created\n",
      "[NbConvertApp] Writing 78363 bytes to /sfs/qumulo/qhome/nle4bz/ds5559/Project/dimensionaModeling.pdf\n",
      "[NbConvertApp] Converting notebook /sfs/qumulo/qhome/nle4bz/ds5559/Project/finalproject.ipynb to pdf\n",
      "[NbConvertApp] Writing 43120 bytes to ./notebook.tex\n",
      "[NbConvertApp] Building PDF\n",
      "[NbConvertApp] Running xelatex 3 times: ['xelatex', './notebook.tex', '-quiet']\n",
      "[NbConvertApp] Running bibtex 1 time: ['bibtex', './notebook']\n",
      "[NbConvertApp] WARNING | bibtex had problems, most likely because there were no citations\n",
      "[NbConvertApp] PDF successfully created\n",
      "[NbConvertApp] Writing 46209 bytes to /sfs/qumulo/qhome/nle4bz/ds5559/Project/finalproject.pdf\n",
      "[NbConvertApp] Converting notebook /sfs/qumulo/qhome/nle4bz/ds5559/Project/initialModeling.ipynb to pdf\n",
      "[NbConvertApp] Writing 40169 bytes to ./notebook.tex\n",
      "[NbConvertApp] Building PDF\n",
      "[NbConvertApp] Running xelatex 3 times: ['xelatex', './notebook.tex', '-quiet']\n",
      "[NbConvertApp] Running bibtex 1 time: ['bibtex', './notebook']\n",
      "[NbConvertApp] WARNING | bibtex had problems, most likely because there were no citations\n",
      "[NbConvertApp] PDF successfully created\n",
      "[NbConvertApp] Writing 48069 bytes to /sfs/qumulo/qhome/nle4bz/ds5559/Project/initialModeling.pdf\n",
      "[NbConvertApp] Converting notebook /sfs/qumulo/qhome/nle4bz/ds5559/Project/paidClassifier.ipynb to pdf\n",
      "[NbConvertApp] Writing 75422 bytes to ./notebook.tex\n",
      "[NbConvertApp] Building PDF\n",
      "[NbConvertApp] Running xelatex 3 times: ['xelatex', './notebook.tex', '-quiet']\n",
      "[NbConvertApp] Running bibtex 1 time: ['bibtex', './notebook']\n",
      "[NbConvertApp] WARNING | bibtex had problems, most likely because there were no citations\n",
      "[NbConvertApp] PDF successfully created\n",
      "[NbConvertApp] Writing 74505 bytes to /sfs/qumulo/qhome/nle4bz/ds5559/Project/paidClassifier.pdf\n",
      "[NbConvertApp] Converting notebook /sfs/qumulo/qhome/nle4bz/ds5559/Project/tipAmount (1).ipynb to pdf\n",
      "[NbConvertApp] Support files will be in tipAmount (1)_files/\n",
      "[NbConvertApp] Making directory ./tipAmount (1)_files\n",
      "[NbConvertApp] Writing 85034 bytes to ./notebook.tex\n",
      "[NbConvertApp] Building PDF\n",
      "[NbConvertApp] Running xelatex 3 times: ['xelatex', './notebook.tex', '-quiet']\n",
      "[NbConvertApp] Running bibtex 1 time: ['bibtex', './notebook']\n",
      "[NbConvertApp] WARNING | bibtex had problems, most likely because there were no citations\n",
      "[NbConvertApp] PDF successfully created\n",
      "[NbConvertApp] Writing 91293 bytes to /sfs/qumulo/qhome/nle4bz/ds5559/Project/tipAmount (1).pdf\n",
      "[NbConvertApp] Converting notebook /sfs/qumulo/qhome/nle4bz/ds5559/Project/tipAmount.ipynb to pdf\n",
      "[NbConvertApp] Support files will be in tipAmount_files/\n",
      "[NbConvertApp] Making directory ./tipAmount_files\n",
      "[NbConvertApp] Writing 65631 bytes to ./notebook.tex\n",
      "[NbConvertApp] Building PDF\n",
      "[NbConvertApp] Running xelatex 3 times: ['xelatex', './notebook.tex', '-quiet']\n",
      "[NbConvertApp] Running bibtex 1 time: ['bibtex', './notebook']\n",
      "[NbConvertApp] WARNING | bibtex had problems, most likely because there were no citations\n",
      "[NbConvertApp] PDF successfully created\n",
      "[NbConvertApp] Writing 99805 bytes to /sfs/qumulo/qhome/nle4bz/ds5559/Project/tipAmount.pdf\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to pdf `pwd`/*.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS 5559",
   "language": "python",
   "name": "ds5559"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
